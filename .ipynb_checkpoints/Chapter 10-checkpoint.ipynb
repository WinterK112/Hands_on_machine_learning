{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "X = iris.data[:, (2,3)] # petal length, petal width\n",
    "y = (iris.target == 0).astype(np.int) #iris setosa?\n",
    "\n",
    "per_clf = Perceptron(max_iter=1000, tol=1e-3, random_state=42)\n",
    "per_clf.fit(X, y)\n",
    "\n",
    "y_pred = per_clf.predict([[2, 0.5]])\n",
    "y_pred "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.7.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.7.0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) =fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# image is represented as 28 x 28 array, not 1D array of size 784\n",
    "\n",
    "X_train_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('uint8')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_full.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating validation set\n",
    "\n",
    "X_valid, X_train = X_train_full[:5000] / 255.0, X_train_full[5000:] / 255.0\n",
    "y_valid, y_train = y_train_full[:5000] / 255.0, y_train_full[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Coat'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names = [\"T-shirts/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \n",
    "               \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
    "\n",
    "class_names[y_train[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building neural network\n",
    "\n",
    "model = keras.models.Sequential() # creates a Sequential model\n",
    "model.add(keras.layers.Flatten(input_shape = [28, 28])) \n",
    "# first layer, role = convert each input image into 1D array\n",
    "# if it receives input data X, it computes X.reshape(-1, 1)\n",
    "# first layer in the model --> should specify the input_shape\n",
    "\n",
    "model.add(keras.layers.Dense(300, activation = \"relu\"))\n",
    "# add a Dense hidden layers with 300 neurons\n",
    "# use ReLU activation function\n",
    "# each Dense layer manages its own weight matrix, containing all the connection weights between the neurons and\n",
    "# and their inputs\n",
    "\n",
    "model.add(keras.layers.Dense(100, activation = \"relu\"))\n",
    "# add second Dense hidden layer\n",
    "# 100 neurons with \"ReLU\" activation function\n",
    "\n",
    "model.add(keras.layers.Dense(10, activation = \"softmax\"))\n",
    "# add Dense output layer iwth 10 neurons (one per class) using softmax activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or passing list of layers\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape = [28, 28]),\n",
    "    keras.layers.Dense(300, activation = \"relu\"),\n",
    "    keras.layers.Dense(100, activation = \"relu\"),\n",
    "    keras.layers.Dense(10, activation = \"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_1 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 300)               235500    \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 100)               30100     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.layers.core.flatten.Flatten at 0x7fafdcdc9d60>,\n",
       " <keras.layers.core.dense.Dense at 0x7fafdcdc90d0>,\n",
       " <keras.layers.core.dense.Dense at 0x7fb005906a60>,\n",
       " <keras.layers.core.dense.Dense at 0x7fafdcf9d070>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dense_3'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden1 = model.layers[1]\n",
    "hidden1.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_layer(hidden1.name) is hidden1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.02496208, -0.04940628,  0.07146882, ...,  0.02377096,\n",
       "         0.07285054, -0.01146747],\n",
       "       [ 0.00221005, -0.00909812, -0.03824003, ..., -0.06000178,\n",
       "        -0.02435223, -0.05681328],\n",
       "       [-0.04154874,  0.0215659 , -0.00463859, ...,  0.00826827,\n",
       "         0.02004173,  0.0255536 ],\n",
       "       ...,\n",
       "       [-0.00872287, -0.04927643,  0.02153963, ...,  0.01937277,\n",
       "         0.04993489, -0.02485989],\n",
       "       [ 0.0048048 , -0.06851169, -0.01498412, ..., -0.02620291,\n",
       "         0.01518889,  0.07100444],\n",
       "       [-0.02057203,  0.05424425,  0.06417654, ..., -0.0555786 ,\n",
       "         0.01843219,  0.07276711]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights, biases = hidden1.get_weights()\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 300)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biases.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "\n",
    "model.compile(loss = \"sparse_categorical_crossentropy\", # we have sparse labels\n",
    "             optimizer = \"sgd\", # train using simple stochastic Gradient Descent\n",
    "             metrics = [\"accuracy\"])\n",
    "\n",
    "# or \n",
    "\n",
    "model.compile(loss=keras.losses.sparse_categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.SGD(),\n",
    "              metrics=[keras.metrics.sparse_categorical_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.7100 - sparse_categorical_accuracy: 0.7679 - val_loss: 7.1695 - val_sparse_categorical_accuracy: 0.0614\n",
      "Epoch 2/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.4887 - sparse_categorical_accuracy: 0.8294 - val_loss: 6.9415 - val_sparse_categorical_accuracy: 0.0746\n",
      "Epoch 3/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4455 - sparse_categorical_accuracy: 0.8441 - val_loss: 7.5785 - val_sparse_categorical_accuracy: 0.0734\n",
      "Epoch 4/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4189 - sparse_categorical_accuracy: 0.8520 - val_loss: 7.1441 - val_sparse_categorical_accuracy: 0.0770\n",
      "Epoch 5/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3996 - sparse_categorical_accuracy: 0.8592 - val_loss: 7.7052 - val_sparse_categorical_accuracy: 0.0752\n",
      "Epoch 6/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3830 - sparse_categorical_accuracy: 0.8654 - val_loss: 7.7600 - val_sparse_categorical_accuracy: 0.0758\n",
      "Epoch 7/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3693 - sparse_categorical_accuracy: 0.8689 - val_loss: 7.6540 - val_sparse_categorical_accuracy: 0.0764\n",
      "Epoch 8/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3574 - sparse_categorical_accuracy: 0.8732 - val_loss: 7.6188 - val_sparse_categorical_accuracy: 0.0788\n",
      "Epoch 9/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3467 - sparse_categorical_accuracy: 0.8764 - val_loss: 7.8274 - val_sparse_categorical_accuracy: 0.0804\n",
      "Epoch 10/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3373 - sparse_categorical_accuracy: 0.8800 - val_loss: 8.1130 - val_sparse_categorical_accuracy: 0.0764\n",
      "Epoch 11/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3276 - sparse_categorical_accuracy: 0.8828 - val_loss: 7.4969 - val_sparse_categorical_accuracy: 0.0840\n",
      "Epoch 12/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3204 - sparse_categorical_accuracy: 0.8845 - val_loss: 7.5528 - val_sparse_categorical_accuracy: 0.0822\n",
      "Epoch 13/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3130 - sparse_categorical_accuracy: 0.8879 - val_loss: 8.4412 - val_sparse_categorical_accuracy: 0.0800\n",
      "Epoch 14/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3065 - sparse_categorical_accuracy: 0.8894 - val_loss: 8.6235 - val_sparse_categorical_accuracy: 0.0704\n",
      "Epoch 15/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3004 - sparse_categorical_accuracy: 0.8917 - val_loss: 8.5252 - val_sparse_categorical_accuracy: 0.0648\n",
      "Epoch 16/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2919 - sparse_categorical_accuracy: 0.8943 - val_loss: 8.0010 - val_sparse_categorical_accuracy: 0.0854\n",
      "Epoch 17/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2862 - sparse_categorical_accuracy: 0.8965 - val_loss: 8.5346 - val_sparse_categorical_accuracy: 0.0626\n",
      "Epoch 18/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2818 - sparse_categorical_accuracy: 0.8980 - val_loss: 8.9815 - val_sparse_categorical_accuracy: 0.0814\n",
      "Epoch 19/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2762 - sparse_categorical_accuracy: 0.8997 - val_loss: 8.3807 - val_sparse_categorical_accuracy: 0.0778\n",
      "Epoch 20/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2702 - sparse_categorical_accuracy: 0.9029 - val_loss: 8.8015 - val_sparse_categorical_accuracy: 0.0772\n",
      "Epoch 21/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2648 - sparse_categorical_accuracy: 0.9045 - val_loss: 9.2808 - val_sparse_categorical_accuracy: 0.0706\n",
      "Epoch 22/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2614 - sparse_categorical_accuracy: 0.9060 - val_loss: 8.7510 - val_sparse_categorical_accuracy: 0.0842\n",
      "Epoch 23/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2555 - sparse_categorical_accuracy: 0.9074 - val_loss: 8.9760 - val_sparse_categorical_accuracy: 0.0788\n",
      "Epoch 24/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2509 - sparse_categorical_accuracy: 0.9097 - val_loss: 8.3557 - val_sparse_categorical_accuracy: 0.0832\n",
      "Epoch 25/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2480 - sparse_categorical_accuracy: 0.9102 - val_loss: 8.7725 - val_sparse_categorical_accuracy: 0.0842\n",
      "Epoch 26/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2419 - sparse_categorical_accuracy: 0.9122 - val_loss: 9.2419 - val_sparse_categorical_accuracy: 0.0792\n",
      "Epoch 27/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2386 - sparse_categorical_accuracy: 0.9132 - val_loss: 10.1278 - val_sparse_categorical_accuracy: 0.0758\n",
      "Epoch 28/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2341 - sparse_categorical_accuracy: 0.9146 - val_loss: 9.4322 - val_sparse_categorical_accuracy: 0.0798\n",
      "Epoch 29/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2311 - sparse_categorical_accuracy: 0.9162 - val_loss: 9.9510 - val_sparse_categorical_accuracy: 0.0728\n",
      "Epoch 30/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2267 - sparse_categorical_accuracy: 0.9181 - val_loss: 9.5029 - val_sparse_categorical_accuracy: 0.0814\n"
     ]
    }
   ],
   "source": [
    "# Training and evaluating model\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs = 30,\n",
    "                    validation_data = (X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 72.8639 - sparse_categorical_accuracy: 0.8400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[72.86392211914062, 0.8399999737739563]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using the model to make predictions\n",
    "\n",
    "X_new = X_test[:3]\n",
    "y_proba = model.predict(X_new)\n",
    "y_proba.round(2)\n",
    "\n",
    "# 100 % being ankle boot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 2, 1])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = np.argmax(model.predict(X_new), axis=-1)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Ankle boot', 'Pullover', 'Trouser'], dtype='<U12')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(class_names)[y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 2, 1], dtype=uint8)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_new = y_test[:3]\n",
    "y_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regrssion MLP using Sequential API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = fetch_california_housing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    housing.data, housing.target)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full, y_train_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 1.1961 - val_loss: 0.5906\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5718 - val_loss: 0.5054\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4953 - val_loss: 0.4710\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4676 - val_loss: 0.4516\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4551 - val_loss: 0.4406\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 978us/step - loss: 0.4468 - val_loss: 0.4324\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4366 - val_loss: 0.4284\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 956us/step - loss: 0.4366 - val_loss: 0.4202\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4253 - val_loss: 0.4194\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4209 - val_loss: 0.4132\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 993us/step - loss: 0.4180 - val_loss: 0.4090\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4122 - val_loss: 0.4048\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 972us/step - loss: 0.4076 - val_loss: 0.4002\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4024 - val_loss: 0.3985\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 990us/step - loss: 0.4012 - val_loss: 0.3939\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4033 - val_loss: 0.3903\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3943 - val_loss: 0.3974\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 951us/step - loss: 0.3973 - val_loss: 0.3948\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 984us/step - loss: 0.3882 - val_loss: 0.3851\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3863 - val_loss: 0.3803\n",
      "162/162 [==============================] - 0s 644us/step - loss: 0.4049\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation = \"relu\", input_shape = X_train.shape[1:]),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss = \"mean_squared_error\", optimizer = \"sgd\")\n",
    "history = model.fit(X_train, y_train, epochs = 20,\n",
    "                   validation_data = (X_valid, y_valid))\n",
    "mse_test = model.evaluate(X_test, y_test)\n",
    "X_new = X_test[:3] # pretend these are new instanes\n",
    "y_pred = model.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "plt.plot(pd.DataFrame(history.history))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = keras.layers.Input(shape = X_train.shape[1:])\n",
    "# create input object, specification of the kind of input the model will get, including shape and dtype\n",
    "\n",
    "hidden1 = keras.layers.Dense(30, activation = \"relu\")(input_)\n",
    "# Dense layers with 30 neurons, using ReLU activation function\n",
    "\n",
    "hidden2 = keras.layers.Dense(30, activation = \"relu\")(hidden1)\n",
    "# create second layers, again we use it as a function\n",
    "\n",
    "concat = keras.layers.Concatenate()([input_, hidden2])\n",
    "# use Concatenate layer\n",
    "\n",
    "output = keras.layers.Dense(1)(concat)\n",
    "# create ouput layer, with a single neuron and no activation function\n",
    "\n",
    "model = keras.Model(inputs = [input_], outputs = [output])\n",
    "# Create a Keras model, specifying which inputs and outputs to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# five featrues through the wide path (features 0 to 4) --> input_A\n",
    "# six features through the deep path (features 2 to 7) --> input_B\n",
    "\n",
    "input_A = keras.layers.Input(shape = [5], name = \"wide_input\")\n",
    "input_B = keras.layers.Input(shape = [6], name = \"deep_input\")\n",
    "hidden1 = keras.layers.Dense(30, activation = \"relu\")(input_B)\n",
    "hidden2 = keras.layers.Dense(30, activation = \"relu\")(hidden1)\n",
    "concat = keras.layers.concatenate([input_A, hidden2])\n",
    "output = keras.layers.Dense(1, name = \"output\")(concat)\n",
    "model = keras.Model(inputs = [input_A, input_B], outputs = [output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4773 - val_loss: 0.4586\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4743 - val_loss: 0.4560\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4718 - val_loss: 0.4536\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4691 - val_loss: 0.4515\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4671 - val_loss: 0.4490\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4651 - val_loss: 0.4474\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4630 - val_loss: 0.4458\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4613 - val_loss: 0.4439\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4593 - val_loss: 0.4426\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4577 - val_loss: 0.4413\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4559 - val_loss: 0.4404\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4546 - val_loss: 0.4388\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4531 - val_loss: 0.4373\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4516 - val_loss: 0.4359\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4501 - val_loss: 0.4348\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4490 - val_loss: 0.4337\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4477 - val_loss: 0.4330\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4465 - val_loss: 0.4319\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4450 - val_loss: 0.4309\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4443 - val_loss: 0.4296\n",
      "162/162 [==============================] - 0s 1ms/step - loss: 0.4708\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss = \"mse\", optimizer = keras.optimizers.SGD(learning_rate = 1e-3))\n",
    "\n",
    "X_train_A, X_train_B = X_train[:, :5], X_train[:, 2:]\n",
    "X_valid_A, X_valid_B = X_valid[:, :5], X_valid[:, 2:]\n",
    "X_test_A, X_test_B = X_test[:, :5], X_test[:, 2:]\n",
    "X_new_A, X_new_B = X_test_A[:3], X_test_B[:3]\n",
    "\n",
    "history = model.fit((X_train_A, X_train_B), y_train, epochs =20,\n",
    "                   validation_data = ((X_valid_A, X_valid_B), y_valid))\n",
    "mse_test = model.evaluate((X_test_A, X_test_B), y_test)\n",
    "y_pred = model.predict((X_new_A, X_new_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding extra layer\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "input_A = keras.layers.Input(shape=[5], name=\"wide_input\")\n",
    "input_B = keras.layers.Input(shape=[6], name=\"deep_input\")\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_B)\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "concat = keras.layers.concatenate([input_A, hidden2])\n",
    "\n",
    "output = keras.layers.Dense(1, name = \"main_output\")(concat)\n",
    "aux_output = keras.layers.Dense(1, name = \"aux_output\")(hidden2)\n",
    "model = keras.Model(inputs = [input_A, input_B], outputs = [output, aux_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.9688 - main_output_loss: 0.8593 - aux_output_loss: 1.9541 - val_loss: 0.5774 - val_main_output_loss: 0.5131 - val_aux_output_loss: 1.1560\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5685 - main_output_loss: 0.5075 - aux_output_loss: 1.1170 - val_loss: 0.5209 - val_main_output_loss: 0.4697 - val_aux_output_loss: 0.9822\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5263 - main_output_loss: 0.4774 - aux_output_loss: 0.9670 - val_loss: 0.4943 - val_main_output_loss: 0.4532 - val_aux_output_loss: 0.8644\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5206 - main_output_loss: 0.4833 - aux_output_loss: 0.8566 - val_loss: 0.4736 - val_main_output_loss: 0.4400 - val_aux_output_loss: 0.7765\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4854 - main_output_loss: 0.4520 - aux_output_loss: 0.7858 - val_loss: 0.4719 - val_main_output_loss: 0.4442 - val_aux_output_loss: 0.7210\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4875 - main_output_loss: 0.4598 - aux_output_loss: 0.7365 - val_loss: 0.4493 - val_main_output_loss: 0.4234 - val_aux_output_loss: 0.6830\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4623 - main_output_loss: 0.4353 - aux_output_loss: 0.7059 - val_loss: 0.4383 - val_main_output_loss: 0.4140 - val_aux_output_loss: 0.6573\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4517 - main_output_loss: 0.4263 - aux_output_loss: 0.6807 - val_loss: 0.4387 - val_main_output_loss: 0.4161 - val_aux_output_loss: 0.6424\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4779 - main_output_loss: 0.4562 - aux_output_loss: 0.6735 - val_loss: 0.4242 - val_main_output_loss: 0.4017 - val_aux_output_loss: 0.6264\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4434 - main_output_loss: 0.4203 - aux_output_loss: 0.6510 - val_loss: 0.4175 - val_main_output_loss: 0.3955 - val_aux_output_loss: 0.6148\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4570 - main_output_loss: 0.4363 - aux_output_loss: 0.6427 - val_loss: 0.4148 - val_main_output_loss: 0.3933 - val_aux_output_loss: 0.6082\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4273 - main_output_loss: 0.4044 - aux_output_loss: 0.6334 - val_loss: 0.4096 - val_main_output_loss: 0.3894 - val_aux_output_loss: 0.5913\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4209 - main_output_loss: 0.3988 - aux_output_loss: 0.6197 - val_loss: 0.3965 - val_main_output_loss: 0.3762 - val_aux_output_loss: 0.5795\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4185 - main_output_loss: 0.3974 - aux_output_loss: 0.6088 - val_loss: 0.3896 - val_main_output_loss: 0.3696 - val_aux_output_loss: 0.5696\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4076 - main_output_loss: 0.3864 - aux_output_loss: 0.5982 - val_loss: 0.3946 - val_main_output_loss: 0.3762 - val_aux_output_loss: 0.5603\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4141 - main_output_loss: 0.3931 - aux_output_loss: 0.6033 - val_loss: 0.3812 - val_main_output_loss: 0.3627 - val_aux_output_loss: 0.5479\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4019 - main_output_loss: 0.3823 - aux_output_loss: 0.5787 - val_loss: 0.3744 - val_main_output_loss: 0.3565 - val_aux_output_loss: 0.5353\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4126 - main_output_loss: 0.3946 - aux_output_loss: 0.5750 - val_loss: 0.3695 - val_main_output_loss: 0.3524 - val_aux_output_loss: 0.5230\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3891 - main_output_loss: 0.3703 - aux_output_loss: 0.5581 - val_loss: 0.3652 - val_main_output_loss: 0.3485 - val_aux_output_loss: 0.5153\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3840 - main_output_loss: 0.3656 - aux_output_loss: 0.5495 - val_loss: 0.3656 - val_main_output_loss: 0.3499 - val_aux_output_loss: 0.5076\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss = [\"mse\", \"mse\"], loss_weights = [0.9, 0.1], optimizer = \"sgd\")\n",
    "\n",
    "history = model.fit(\n",
    "    [X_train_A, X_train_B], [y_train, y_train], epochs = 20,\n",
    "    validation_data = ([X_valid_A, X_valid_B], [y_valid, y_valid]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 1ms/step - loss: 0.3989 - main_output_loss: 0.3838 - aux_output_loss: 0.5340\n"
     ]
    }
   ],
   "source": [
    "# evaluating the model\n",
    "\n",
    "total_loss, main_loss, aux_loss = model.evaluate(\n",
    "    [X_test_A, X_test_B], [y_test, y_test])\n",
    "\n",
    "y_pred_main, y_pred_aux = model.predict([X_new_A, X_new_B])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Subclassing API to build Dynamic Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WideAndDeepModel(keras.Model):\n",
    "    def __init__(self, units = 30, activation = \"relu\", **kwargs):\n",
    "        super().__init__(**kwargs) # handles standard args (e.g., name)\n",
    "        self.hidden1 = keras.layers.Dense(units, activation = activation)\n",
    "        self.hidden2 = keras.layers.Dense(units, activation = activation)\n",
    "        self.main_output = keras.layers.Dense(1)\n",
    "        self.main_output = keras.layers.Dense(1)\n",
    "        \n",
    "def call(self, inputs):\n",
    "    input_A, input_B = inputs\n",
    "    hidden1 = self.hidden1(input_B)\n",
    "    hidden2 = self.hidden2(hidden1)\n",
    "    concat = keras.layers.concatenate([input_A, hidden2])\n",
    "    main_output = self.main_output(concat)\n",
    "    aux_output = self.aux_output(hidden2)\n",
    "    return main_output, aux_output\n",
    "\n",
    "model = WideAndDeepModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Restoring a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 1.8550 - val_loss: 0.7399\n",
      "Epoch 2/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.7289 - val_loss: 0.6076\n",
      "Epoch 3/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6117 - val_loss: 0.5686\n",
      "Epoch 4/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5751 - val_loss: 0.5413\n",
      "Epoch 5/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5484 - val_loss: 0.5213\n",
      "Epoch 6/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5272 - val_loss: 0.5039\n",
      "Epoch 7/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5118 - val_loss: 0.4923\n",
      "Epoch 8/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4987 - val_loss: 0.4822\n",
      "Epoch 9/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4885 - val_loss: 0.4740\n",
      "Epoch 10/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4802 - val_loss: 0.4667\n",
      "162/162 [==============================] - 0s 783us/step - loss: 0.5068\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=[8]),\n",
    "    keras.layers.Dense(30, activation=\"relu\"),\n",
    "    keras.layers.Dense(1)\n",
    "])    \n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(learning_rate=1e-3))\n",
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))\n",
    "mse_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"my_keras_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"my_keras_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fc70b075040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2.273878 ],\n",
       "       [0.8512738],\n",
       "       [2.2067485]], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"my_keras_weights.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fc70b0b1970>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights(\"my_keras_weights.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=[8]),\n",
    "    keras.layers.Dense(30, activation=\"relu\"),\n",
    "    keras.layers.Dense(1)\n",
    "])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 1.8550 - val_loss: 0.7399\n",
      "Epoch 2/10\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.7289 - val_loss: 0.6076\n",
      "Epoch 3/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6117 - val_loss: 0.5686\n",
      "Epoch 4/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5751 - val_loss: 0.5413\n",
      "Epoch 5/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5484 - val_loss: 0.5213\n",
      "Epoch 6/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5272 - val_loss: 0.5039\n",
      "Epoch 7/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5118 - val_loss: 0.4923\n",
      "Epoch 8/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4987 - val_loss: 0.4822\n",
      "Epoch 9/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4885 - val_loss: 0.4740\n",
      "Epoch 10/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4802 - val_loss: 0.4667\n",
      "162/162 [==============================] - 0s 827us/step - loss: 0.5068\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(learning_rate=1e-3))\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_keras_model.h5\", save_best_only=True)\n",
    "# ModelCheckpoint saves checkpoints of model at regular intervals during training at the end of each epoch\n",
    "# save_best_only=True save model when its performance on the validation is the best so far\n",
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[checkpoint_cb])\n",
    "model = keras.models.load_model(\"my_keras_model.h5\") # rollback to best model\n",
    "mse_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4737 - val_loss: 0.4616\n",
      "Epoch 2/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4681 - val_loss: 0.4568\n",
      "Epoch 3/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4636 - val_loss: 0.4533\n",
      "Epoch 4/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4594 - val_loss: 0.4500\n",
      "Epoch 5/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4559 - val_loss: 0.4472\n",
      "Epoch 6/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4523 - val_loss: 0.4434\n",
      "Epoch 7/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4502 - val_loss: 0.4418\n",
      "Epoch 8/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4471 - val_loss: 0.4399\n",
      "Epoch 9/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4449 - val_loss: 0.4377\n",
      "Epoch 10/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4428 - val_loss: 0.4354\n",
      "Epoch 11/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4407 - val_loss: 0.4337\n",
      "Epoch 12/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4386 - val_loss: 0.4325\n",
      "Epoch 13/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4371 - val_loss: 0.4306\n",
      "Epoch 14/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4355 - val_loss: 0.4290\n",
      "Epoch 15/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4335 - val_loss: 0.4281\n",
      "Epoch 16/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4322 - val_loss: 0.4267\n",
      "Epoch 17/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4307 - val_loss: 0.4249\n",
      "Epoch 18/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4294 - val_loss: 0.4236\n",
      "Epoch 19/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4275 - val_loss: 0.4228\n",
      "Epoch 20/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4265 - val_loss: 0.4212\n",
      "Epoch 21/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4254 - val_loss: 0.4198\n",
      "Epoch 22/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4240 - val_loss: 0.4195\n",
      "Epoch 23/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4233 - val_loss: 0.4185\n",
      "Epoch 24/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4217 - val_loss: 0.4167\n",
      "Epoch 25/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4203 - val_loss: 0.4155\n",
      "Epoch 26/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4194 - val_loss: 0.4145\n",
      "Epoch 27/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4184 - val_loss: 0.4133\n",
      "Epoch 28/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4173 - val_loss: 0.4126\n",
      "Epoch 29/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4161 - val_loss: 0.4118\n",
      "Epoch 30/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4151 - val_loss: 0.4117\n",
      "Epoch 31/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4146 - val_loss: 0.4106\n",
      "Epoch 32/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4128 - val_loss: 0.4089\n",
      "Epoch 33/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4120 - val_loss: 0.4088\n",
      "Epoch 34/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4106 - val_loss: 0.4072\n",
      "Epoch 35/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4103 - val_loss: 0.4079\n",
      "Epoch 36/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4092 - val_loss: 0.4058\n",
      "Epoch 37/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4080 - val_loss: 0.4043\n",
      "Epoch 38/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4071 - val_loss: 0.4035\n",
      "Epoch 39/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4066 - val_loss: 0.4025\n",
      "Epoch 40/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4050 - val_loss: 0.4026\n",
      "Epoch 41/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4043 - val_loss: 0.4011\n",
      "Epoch 42/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4037 - val_loss: 0.4003\n",
      "Epoch 43/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4025 - val_loss: 0.4005\n",
      "Epoch 44/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4017 - val_loss: 0.3989\n",
      "Epoch 45/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4006 - val_loss: 0.3980\n",
      "Epoch 46/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3998 - val_loss: 0.3975\n",
      "Epoch 47/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3991 - val_loss: 0.3960\n",
      "Epoch 48/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3982 - val_loss: 0.3946\n",
      "Epoch 49/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3973 - val_loss: 0.3943\n",
      "Epoch 50/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3963 - val_loss: 0.3937\n",
      "Epoch 51/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3957 - val_loss: 0.3935\n",
      "Epoch 52/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3947 - val_loss: 0.3916\n",
      "Epoch 53/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3946 - val_loss: 0.3910\n",
      "Epoch 54/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3935 - val_loss: 0.3902\n",
      "Epoch 55/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3923 - val_loss: 0.3906\n",
      "Epoch 56/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3922 - val_loss: 0.3888\n",
      "Epoch 57/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3904 - val_loss: 0.3882\n",
      "Epoch 58/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3898 - val_loss: 0.3880\n",
      "Epoch 59/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3888 - val_loss: 0.3864\n",
      "Epoch 60/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3881 - val_loss: 0.3869\n",
      "Epoch 61/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3875 - val_loss: 0.3852\n",
      "Epoch 62/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3862 - val_loss: 0.3850\n",
      "Epoch 63/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3858 - val_loss: 0.3837\n",
      "Epoch 64/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3855 - val_loss: 0.3832\n",
      "Epoch 65/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3842 - val_loss: 0.3824\n",
      "Epoch 66/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3833 - val_loss: 0.3824\n",
      "Epoch 67/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3825 - val_loss: 0.3814\n",
      "Epoch 68/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3825 - val_loss: 0.3801\n",
      "Epoch 69/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3812 - val_loss: 0.3793\n",
      "Epoch 70/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3809 - val_loss: 0.3789\n",
      "Epoch 71/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3797 - val_loss: 0.3787\n",
      "Epoch 72/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3802 - val_loss: 0.3777\n",
      "Epoch 73/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3798 - val_loss: 0.3770\n",
      "Epoch 74/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3775 - val_loss: 0.3767\n",
      "Epoch 75/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3771 - val_loss: 0.3753\n",
      "Epoch 76/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3767 - val_loss: 0.3754\n",
      "Epoch 77/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3753 - val_loss: 0.3746\n",
      "Epoch 78/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3748 - val_loss: 0.3741\n",
      "Epoch 79/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3747 - val_loss: 0.3728\n",
      "Epoch 80/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3729 - val_loss: 0.3727\n",
      "Epoch 81/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3729 - val_loss: 0.3713\n",
      "Epoch 82/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3724 - val_loss: 0.3713\n",
      "Epoch 83/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3715 - val_loss: 0.3711\n",
      "Epoch 84/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3709 - val_loss: 0.3696\n",
      "Epoch 85/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3707 - val_loss: 0.3693\n",
      "Epoch 86/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3696 - val_loss: 0.3694\n",
      "Epoch 87/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3689 - val_loss: 0.3686\n",
      "Epoch 88/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3693 - val_loss: 0.3678\n",
      "Epoch 89/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3682 - val_loss: 0.3677\n",
      "Epoch 90/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3668 - val_loss: 0.3665\n",
      "Epoch 91/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3661 - val_loss: 0.3653\n",
      "Epoch 92/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3657 - val_loss: 0.3650\n",
      "Epoch 93/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3661 - val_loss: 0.3657\n",
      "Epoch 94/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3640 - val_loss: 0.3648\n",
      "Epoch 95/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3645 - val_loss: 0.3636\n",
      "Epoch 96/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3625 - val_loss: 0.3631\n",
      "Epoch 97/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3633 - val_loss: 0.3634\n",
      "Epoch 98/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3616 - val_loss: 0.3616\n",
      "Epoch 99/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3626 - val_loss: 0.3612\n",
      "Epoch 100/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3603 - val_loss: 0.3610\n"
     ]
    }
   ],
   "source": [
    "# early stopping\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(learning_rate=1e-3))\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience = 10, restore_best_weights = True)\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs = 100,\n",
    "                   validation_data = (X_valid, y_valid),\n",
    "                   callbacks = [checkpoint_cb, early_stopping_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrintValTrainRatioCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        print(\"\\nval/train: {:.2f}\".format(logs[\"val_loss\"] / logs[\"loss\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "360/363 [============================>.] - ETA: 0s - loss: 0.3597\n",
      "val/train: 1.00\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3603 - val_loss: 0.3606\n"
     ]
    }
   ],
   "source": [
    "val_train_ratio_cb = PrintValTrainRatioCallback()\n",
    "history = model.fit(X_train, y_train, epochs=1,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[val_train_ratio_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorBoard for Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "root_logdir = os.path.join(os.curdir, \"my_logs\")\n",
    "\n",
    "def get_run_logdir():\n",
    "    import time\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%_S\")\n",
    "    return os.path.join(root_logdir, run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./my_logs/run_2021_12_27-16_28_38'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_logdir = get_run_logdir()\n",
    "run_logdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=[8]),\n",
    "    keras.layers.Dense(30, activation=\"relu\"),\n",
    "    keras.layers.Dense(1)\n",
    "])    \n",
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(learning_rate=1e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 1.8550 - val_loss: 0.7399\n",
      "Epoch 2/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.7289 - val_loss: 0.6076\n",
      "Epoch 3/30\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.6117 - val_loss: 0.5686\n",
      "Epoch 4/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5751 - val_loss: 0.5413\n",
      "Epoch 5/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5484 - val_loss: 0.5213\n",
      "Epoch 6/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5272 - val_loss: 0.5039\n",
      "Epoch 7/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5118 - val_loss: 0.4923\n",
      "Epoch 8/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4987 - val_loss: 0.4822\n",
      "Epoch 9/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4885 - val_loss: 0.4740\n",
      "Epoch 10/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4802 - val_loss: 0.4667\n",
      "Epoch 11/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4737 - val_loss: 0.4616\n",
      "Epoch 12/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4680 - val_loss: 0.4573\n",
      "Epoch 13/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4635 - val_loss: 0.4530\n",
      "Epoch 14/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4594 - val_loss: 0.4491\n",
      "Epoch 15/30\n",
      "363/363 [==============================] - 0s 988us/step - loss: 0.4556 - val_loss: 0.4467\n",
      "Epoch 16/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4526 - val_loss: 0.4440\n",
      "Epoch 17/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4497 - val_loss: 0.4414\n",
      "Epoch 18/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4473 - val_loss: 0.4392\n",
      "Epoch 19/30\n",
      "363/363 [==============================] - 0s 996us/step - loss: 0.4445 - val_loss: 0.4378\n",
      "Epoch 20/30\n",
      "363/363 [==============================] - 0s 987us/step - loss: 0.4426 - val_loss: 0.4354\n",
      "Epoch 21/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4409 - val_loss: 0.4335\n",
      "Epoch 22/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4390 - val_loss: 0.4329\n",
      "Epoch 23/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4377 - val_loss: 0.4312\n",
      "Epoch 24/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4353 - val_loss: 0.4291\n",
      "Epoch 25/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4337 - val_loss: 0.4276\n",
      "Epoch 26/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4322 - val_loss: 0.4263\n",
      "Epoch 27/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4307 - val_loss: 0.4246\n",
      "Epoch 28/30\n",
      "363/363 [==============================] - 0s 991us/step - loss: 0.4293 - val_loss: 0.4236\n",
      "Epoch 29/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4279 - val_loss: 0.4227\n",
      "Epoch 30/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4266 - val_loss: 0.4222\n"
     ]
    }
   ],
   "source": [
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "history = model.fit(X_train, y_train, epochs = 30,\n",
    "                   validation_data = (X_valid, y_valid),\n",
    "                   callbacks = [tensorboard_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-28c7d7556c2490fe\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-28c7d7556c2490fe\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=./my_logs --port=6006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find-Tuning Neural Network Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function that will build and compile a Keras model\n",
    "\n",
    "def build_model(n_hidden = 1, n_neurons = 30, learning_rate = 3e-3, input_shape =[8]):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.InputLayer(input_shape = input_shape))\n",
    "    for layer in range(n_hidden):\n",
    "        model.add(keras.layers.Dense(n_neurons, activation = \"relu\"))\n",
    "    model.add(keras.layers.Dense(1))\n",
    "    optimizer = keras.optimizers.SGD(lr = learning_rate)\n",
    "    model.compile(loss = \"mse\", optimizer = optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-65-ac588ad3b532>:3: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead.\n",
      "  keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)\n"
     ]
    }
   ],
   "source": [
    "# create KerasRegressor\n",
    "\n",
    "keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 1.1764 - val_loss: 0.6553\n",
      "Epoch 2/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6275 - val_loss: 0.5683\n",
      "Epoch 3/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5607 - val_loss: 0.5212\n",
      "Epoch 4/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5170 - val_loss: 0.4856\n",
      "Epoch 5/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4928 - val_loss: 0.4662\n",
      "Epoch 6/100\n",
      "363/363 [==============================] - 0s 991us/step - loss: 0.4738 - val_loss: 0.4526\n",
      "Epoch 7/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4634 - val_loss: 0.4442\n",
      "Epoch 8/100\n",
      "363/363 [==============================] - 0s 994us/step - loss: 0.4543 - val_loss: 0.4383\n",
      "Epoch 9/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4479 - val_loss: 0.4333\n",
      "Epoch 10/100\n",
      "363/363 [==============================] - 0s 993us/step - loss: 0.4432 - val_loss: 0.4291\n",
      "Epoch 11/100\n",
      "363/363 [==============================] - 0s 992us/step - loss: 0.4388 - val_loss: 0.4252\n",
      "Epoch 12/100\n",
      "363/363 [==============================] - 0s 998us/step - loss: 0.4351 - val_loss: 0.4235\n",
      "Epoch 13/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4328 - val_loss: 0.4207\n",
      "Epoch 14/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4311 - val_loss: 0.4178\n",
      "Epoch 15/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4269 - val_loss: 0.4160\n",
      "Epoch 16/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4251 - val_loss: 0.4137\n",
      "Epoch 17/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4231 - val_loss: 0.4115\n",
      "Epoch 18/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4208 - val_loss: 0.4093\n",
      "Epoch 19/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4180 - val_loss: 0.4080\n",
      "Epoch 20/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4163 - val_loss: 0.4060\n",
      "Epoch 21/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4149 - val_loss: 0.4038\n",
      "Epoch 22/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4130 - val_loss: 0.4029\n",
      "Epoch 23/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4114 - val_loss: 0.4024\n",
      "Epoch 24/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4089 - val_loss: 0.4000\n",
      "Epoch 25/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4076 - val_loss: 0.3985\n",
      "Epoch 26/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4060 - val_loss: 0.3976\n",
      "Epoch 27/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4041 - val_loss: 0.3956\n",
      "Epoch 28/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4028 - val_loss: 0.3947\n",
      "Epoch 29/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4013 - val_loss: 0.3965\n",
      "Epoch 30/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4010 - val_loss: 0.3943\n",
      "Epoch 31/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3990 - val_loss: 0.3930\n",
      "Epoch 32/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3971 - val_loss: 0.3900\n",
      "Epoch 33/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3960 - val_loss: 0.3905\n",
      "Epoch 34/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3944 - val_loss: 0.3881\n",
      "Epoch 35/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3938 - val_loss: 0.3897\n",
      "Epoch 36/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3924 - val_loss: 0.3867\n",
      "Epoch 37/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3906 - val_loss: 0.3851\n",
      "Epoch 38/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3899 - val_loss: 0.3844\n",
      "Epoch 39/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3895 - val_loss: 0.3832\n",
      "Epoch 40/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3877 - val_loss: 0.3843\n",
      "Epoch 41/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3866 - val_loss: 0.3820\n",
      "Epoch 42/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3862 - val_loss: 0.3809\n",
      "Epoch 43/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3846 - val_loss: 0.3809\n",
      "Epoch 44/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3842 - val_loss: 0.3808\n",
      "Epoch 45/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3829 - val_loss: 0.3789\n",
      "Epoch 46/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3825 - val_loss: 0.3781\n",
      "Epoch 47/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3808 - val_loss: 0.3764\n",
      "Epoch 48/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3801 - val_loss: 0.3746\n",
      "Epoch 49/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3791 - val_loss: 0.3740\n",
      "Epoch 50/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3781 - val_loss: 0.3742\n",
      "Epoch 51/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3777 - val_loss: 0.3732\n",
      "Epoch 52/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3763 - val_loss: 0.3726\n",
      "Epoch 53/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3758 - val_loss: 0.3714\n",
      "Epoch 54/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3751 - val_loss: 0.3705\n",
      "Epoch 55/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3739 - val_loss: 0.3729\n",
      "Epoch 56/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3734 - val_loss: 0.3692\n",
      "Epoch 57/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3723 - val_loss: 0.3693\n",
      "Epoch 58/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3714 - val_loss: 0.3697\n",
      "Epoch 59/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3707 - val_loss: 0.3666\n",
      "Epoch 60/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3700 - val_loss: 0.3680\n",
      "Epoch 61/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3693 - val_loss: 0.3658\n",
      "Epoch 62/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3683 - val_loss: 0.3664\n",
      "Epoch 63/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3683 - val_loss: 0.3637\n",
      "Epoch 64/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3673 - val_loss: 0.3645\n",
      "Epoch 65/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3665 - val_loss: 0.3630\n",
      "Epoch 66/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3656 - val_loss: 0.3625\n",
      "Epoch 67/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3650 - val_loss: 0.3624\n",
      "Epoch 68/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3646 - val_loss: 0.3611\n",
      "Epoch 69/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3636 - val_loss: 0.3610\n",
      "Epoch 70/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3639 - val_loss: 0.3605\n",
      "Epoch 71/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3620 - val_loss: 0.3605\n",
      "Epoch 72/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3620 - val_loss: 0.3587\n",
      "Epoch 73/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3614 - val_loss: 0.3592\n",
      "Epoch 74/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3606 - val_loss: 0.3573\n",
      "Epoch 75/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3602 - val_loss: 0.3564\n",
      "Epoch 76/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3593 - val_loss: 0.3566\n",
      "Epoch 77/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3583 - val_loss: 0.3581\n",
      "Epoch 78/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3589 - val_loss: 0.3565\n",
      "Epoch 79/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3576 - val_loss: 0.3547\n",
      "Epoch 80/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3572 - val_loss: 0.3552\n",
      "Epoch 81/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3563 - val_loss: 0.3531\n",
      "Epoch 82/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3561 - val_loss: 0.3531\n",
      "Epoch 83/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3554 - val_loss: 0.3538\n",
      "Epoch 84/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3560 - val_loss: 0.3576\n",
      "Epoch 85/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3545 - val_loss: 0.3519\n",
      "Epoch 86/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3539 - val_loss: 0.3534\n",
      "Epoch 87/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3537 - val_loss: 0.3732\n",
      "Epoch 88/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3539 - val_loss: 0.3502\n",
      "Epoch 89/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3519 - val_loss: 0.3510\n",
      "Epoch 90/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3519 - val_loss: 0.3493\n",
      "Epoch 91/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3511 - val_loss: 0.3485\n",
      "Epoch 92/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3513 - val_loss: 0.3488\n",
      "Epoch 93/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3517 - val_loss: 0.3497\n",
      "Epoch 94/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3524 - val_loss: 0.3475\n",
      "Epoch 95/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3497 - val_loss: 0.3474\n",
      "Epoch 96/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3497 - val_loss: 0.3482\n",
      "Epoch 97/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3493 - val_loss: 0.3486\n",
      "Epoch 98/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3489 - val_loss: 0.3550\n",
      "Epoch 99/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3485 - val_loss: 0.3455\n",
      "Epoch 100/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3470 - val_loss: 0.3455\n",
      "162/162 [==============================] - 0s 763us/step - loss: 0.3690\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fc70b075940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "keras_reg.fit(X_train, y_train, epochs = 100,\n",
    "             validation_data = (X_valid, y_valid),\n",
    "             callbacks = [keras.callbacks.EarlyStopping(patience = 10)])\n",
    "mse_test = keras_reg.score(X_test, y_test)\n",
    "y_pred = keras_reg.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import reciprocal\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_distribs = {\n",
    "    \"n_hidden\": [0, 1, 2, 3],\n",
    "    \"n_neurons\": np.arange(1, 100),\n",
    "    \"learning_rate\": reciprocal(3e-4, 3e-2),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 1ms/step - loss: 4.4057 - val_loss: 1.9872\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3573 - val_loss: 0.9319\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8074 - val_loss: 0.7169\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6875 - val_loss: 0.6618\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6513 - val_loss: 0.6387\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6331 - val_loss: 0.6232\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6199 - val_loss: 0.6105\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6086 - val_loss: 0.5992\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5990 - val_loss: 0.5890\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5903 - val_loss: 0.5798\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5826 - val_loss: 0.5716\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5759 - val_loss: 0.5647\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5697 - val_loss: 0.5579\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5646 - val_loss: 0.5524\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5597 - val_loss: 0.5473\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5555 - val_loss: 0.5426\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5516 - val_loss: 0.5386\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5483 - val_loss: 0.5348\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5453 - val_loss: 0.5315\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5425 - val_loss: 0.5284\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5402 - val_loss: 0.5258\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5381 - val_loss: 0.5234\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5363 - val_loss: 0.5209\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5345 - val_loss: 0.5192\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5330 - val_loss: 0.5171\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5315 - val_loss: 0.5154\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5304 - val_loss: 0.5141\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5290 - val_loss: 0.5127\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5284 - val_loss: 0.5115\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5274 - val_loss: 0.5106\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5266 - val_loss: 0.5094\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5259 - val_loss: 0.5086\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5251 - val_loss: 0.5077\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5246 - val_loss: 0.5070\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5240 - val_loss: 0.5064\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5237 - val_loss: 0.5057\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5229 - val_loss: 0.5055\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5227 - val_loss: 0.5047\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5223 - val_loss: 0.5039\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5221 - val_loss: 0.5037\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5217 - val_loss: 0.5035\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5214 - val_loss: 0.5031\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5211 - val_loss: 0.5028\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5209 - val_loss: 0.5023\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5205 - val_loss: 0.5020\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5206 - val_loss: 0.5017\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5204 - val_loss: 0.5015\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5201 - val_loss: 0.5015\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5200 - val_loss: 0.5013\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5196 - val_loss: 0.5013\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5198 - val_loss: 0.5008\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5197 - val_loss: 0.5005\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5195 - val_loss: 0.5007\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5192 - val_loss: 0.5004\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5196 - val_loss: 0.5002\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5193 - val_loss: 0.5003\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5192 - val_loss: 0.5004\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5192 - val_loss: 0.4999\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5192 - val_loss: 0.4999\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5189 - val_loss: 0.5001\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5188 - val_loss: 0.4999\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5188 - val_loss: 0.4994\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5189 - val_loss: 0.4997\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5188 - val_loss: 0.4995\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5189 - val_loss: 0.4995\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5188 - val_loss: 0.4996\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5187 - val_loss: 0.4996\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5187 - val_loss: 0.4993\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5187 - val_loss: 0.4994\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5187 - val_loss: 0.4992\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5184 - val_loss: 0.4994\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5186 - val_loss: 0.4991\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5183 - val_loss: 0.4989\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5187 - val_loss: 0.4989\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5185 - val_loss: 0.4993\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5186 - val_loss: 0.4991\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5185 - val_loss: 0.4990\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5185 - val_loss: 0.4991\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5185 - val_loss: 0.4992\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5186 - val_loss: 0.4989\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5185 - val_loss: 0.4992\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5184 - val_loss: 0.4988\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5185 - val_loss: 0.4990\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5185 - val_loss: 0.4990\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5185 - val_loss: 0.4990\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5184 - val_loss: 0.4989\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5184 - val_loss: 0.4990\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5184 - val_loss: 0.4989\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5185 - val_loss: 0.4990\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5184 - val_loss: 0.4989\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5184 - val_loss: 0.4989\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5185 - val_loss: 0.4990\n",
      "121/121 [==============================] - 0s 620us/step - loss: 0.5204\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 1ms/step - loss: 4.4030 - val_loss: 1.7377\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.2354 - val_loss: 0.8457\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7478 - val_loss: 0.6676\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 993us/step - loss: 0.6458 - val_loss: 0.6238\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.6160 - val_loss: 0.6053\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 993us/step - loss: 0.6012 - val_loss: 0.5936\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5906 - val_loss: 0.5830\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5819 - val_loss: 0.5734\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5742 - val_loss: 0.5658\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5675 - val_loss: 0.5581\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5617 - val_loss: 0.5517\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 979us/step - loss: 0.5567 - val_loss: 0.5460\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5524 - val_loss: 0.5412\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5486 - val_loss: 0.5377\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5452 - val_loss: 0.5333\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 987us/step - loss: 0.5419 - val_loss: 0.5297\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 995us/step - loss: 0.5399 - val_loss: 0.5269\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.5374 - val_loss: 0.5241\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5355 - val_loss: 0.5215\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 993us/step - loss: 0.5339 - val_loss: 0.5194\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 953us/step - loss: 0.5324 - val_loss: 0.5175\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 957us/step - loss: 0.5313 - val_loss: 0.5161\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 960us/step - loss: 0.5298 - val_loss: 0.5143\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 965us/step - loss: 0.5291 - val_loss: 0.5133\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 965us/step - loss: 0.5281 - val_loss: 0.5124\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 954us/step - loss: 0.5271 - val_loss: 0.5107\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5269 - val_loss: 0.5099\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5260 - val_loss: 0.5094\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5259 - val_loss: 0.5085\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5250 - val_loss: 0.5075\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5251 - val_loss: 0.5075\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 960us/step - loss: 0.5243 - val_loss: 0.5064\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 964us/step - loss: 0.5243 - val_loss: 0.5058\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5242 - val_loss: 0.5053\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 981us/step - loss: 0.5240 - val_loss: 0.5056\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 972us/step - loss: 0.5236 - val_loss: 0.5046\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 0.5233 - val_loss: 0.5050\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 962us/step - loss: 0.5234 - val_loss: 0.5048\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 977us/step - loss: 0.5232 - val_loss: 0.5037\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 956us/step - loss: 0.5230 - val_loss: 0.5041\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 957us/step - loss: 0.5231 - val_loss: 0.5038\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 953us/step - loss: 0.5231 - val_loss: 0.5034\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 960us/step - loss: 0.5227 - val_loss: 0.5033\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.5228 - val_loss: 0.5027\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 970us/step - loss: 0.5227 - val_loss: 0.5024\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.5226 - val_loss: 0.5024\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5227 - val_loss: 0.5026\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 978us/step - loss: 0.5226 - val_loss: 0.5026\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 976us/step - loss: 0.5225 - val_loss: 0.5029\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 951us/step - loss: 0.5227 - val_loss: 0.5026\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 962us/step - loss: 0.5218 - val_loss: 0.5016\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 958us/step - loss: 0.5225 - val_loss: 0.5015\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 986us/step - loss: 0.5223 - val_loss: 0.5020\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.5224 - val_loss: 0.5017\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 979us/step - loss: 0.5223 - val_loss: 0.5014\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 965us/step - loss: 0.5223 - val_loss: 0.5019\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 960us/step - loss: 0.5225 - val_loss: 0.5020\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 956us/step - loss: 0.5225 - val_loss: 0.5014\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 957us/step - loss: 0.5224 - val_loss: 0.5016\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5217 - val_loss: 0.5011\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.5223 - val_loss: 0.5014\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 972us/step - loss: 0.5217 - val_loss: 0.5010\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 965us/step - loss: 0.5226 - val_loss: 0.5009\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 960us/step - loss: 0.5225 - val_loss: 0.5014\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 977us/step - loss: 0.5223 - val_loss: 0.5010\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 969us/step - loss: 0.5225 - val_loss: 0.5009\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 950us/step - loss: 0.5222 - val_loss: 0.5016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 965us/step - loss: 0.5225 - val_loss: 0.5015\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 957us/step - loss: 0.5225 - val_loss: 0.5010\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 974us/step - loss: 0.5221 - val_loss: 0.5008\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 951us/step - loss: 0.5223 - val_loss: 0.5013\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 991us/step - loss: 0.5220 - val_loss: 0.5009\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5223 - val_loss: 0.5008\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5225 - val_loss: 0.5007\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5223 - val_loss: 0.5006\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 960us/step - loss: 0.5223 - val_loss: 0.5009\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 974us/step - loss: 0.5222 - val_loss: 0.5006\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 956us/step - loss: 0.5223 - val_loss: 0.5007\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 947us/step - loss: 0.5217 - val_loss: 0.5009\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 950us/step - loss: 0.5224 - val_loss: 0.5014\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.5225 - val_loss: 0.5013\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 962us/step - loss: 0.5224 - val_loss: 0.5009\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 954us/step - loss: 0.5223 - val_loss: 0.5006\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 956us/step - loss: 0.5221 - val_loss: 0.5005\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 991us/step - loss: 0.5225 - val_loss: 0.5006\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 0.5224 - val_loss: 0.5012\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 964us/step - loss: 0.5221 - val_loss: 0.5004\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 953us/step - loss: 0.5225 - val_loss: 0.5010\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 953us/step - loss: 0.5221 - val_loss: 0.5006\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 953us/step - loss: 0.5226 - val_loss: 0.5009\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 969us/step - loss: 0.5225 - val_loss: 0.5008\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.5221 - val_loss: 0.5005\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5224 - val_loss: 0.5004\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5225 - val_loss: 0.5006\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5225 - val_loss: 0.5005\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5227 - val_loss: 0.5008\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5223 - val_loss: 0.5006\n",
      "121/121 [==============================] - 0s 810us/step - loss: 0.5115\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.5244 - val_loss: 1.8358\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.2953 - val_loss: 0.8950\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7978 - val_loss: 0.7114\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6863 - val_loss: 0.6615\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6480 - val_loss: 0.6380\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6264 - val_loss: 0.6216\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6104 - val_loss: 0.6071\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5972 - val_loss: 0.5944\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5860 - val_loss: 0.5840\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5764 - val_loss: 0.5741\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5682 - val_loss: 0.5654\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5615 - val_loss: 0.5582\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5554 - val_loss: 0.5519\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5502 - val_loss: 0.5467\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5457 - val_loss: 0.5418\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5414 - val_loss: 0.5367\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5382 - val_loss: 0.5325\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5353 - val_loss: 0.5294\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5326 - val_loss: 0.5261\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5305 - val_loss: 0.5236\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5284 - val_loss: 0.5212\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5267 - val_loss: 0.5191\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5249 - val_loss: 0.5167\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5239 - val_loss: 0.5151\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5225 - val_loss: 0.5143\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5215 - val_loss: 0.5123\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5207 - val_loss: 0.5120\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5198 - val_loss: 0.5111\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5193 - val_loss: 0.5096\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5185 - val_loss: 0.5085\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5181 - val_loss: 0.5083\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5176 - val_loss: 0.5074\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5172 - val_loss: 0.5069\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5169 - val_loss: 0.5060\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5165 - val_loss: 0.5060\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5163 - val_loss: 0.5054\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5161 - val_loss: 0.5049\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5154 - val_loss: 0.5051\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5157 - val_loss: 0.5045\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5155 - val_loss: 0.5041\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5153 - val_loss: 0.5036\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5151 - val_loss: 0.5029\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5152 - val_loss: 0.5028\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5149 - val_loss: 0.5030\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5146 - val_loss: 0.5022\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5150 - val_loss: 0.5022\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5147 - val_loss: 0.5025\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5147 - val_loss: 0.5023\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5146 - val_loss: 0.5024\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5146 - val_loss: 0.5023\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5144 - val_loss: 0.5016\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5145 - val_loss: 0.5013\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5145 - val_loss: 0.5020\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5145 - val_loss: 0.5017\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5144 - val_loss: 0.5011\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5145 - val_loss: 0.5017\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5142 - val_loss: 0.5023\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5145 - val_loss: 0.5019\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5145 - val_loss: 0.5014\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5142 - val_loss: 0.5008\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5146 - val_loss: 0.5011\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5143 - val_loss: 0.5008\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5143 - val_loss: 0.5010\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5142 - val_loss: 0.5015\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5140 - val_loss: 0.5003\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5144 - val_loss: 0.5005\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5144 - val_loss: 0.5012\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5143 - val_loss: 0.5014\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5143 - val_loss: 0.5014\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5141 - val_loss: 0.5005\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5144 - val_loss: 0.5009\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5143 - val_loss: 0.5008\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5143 - val_loss: 0.5007\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5142 - val_loss: 0.5006\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5141 - val_loss: 0.5003\n",
      "121/121 [==============================] - 0s 662us/step - loss: 0.5265\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 1ms/step - loss: 1.4340 - val_loss: 0.6940\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6379 - val_loss: 0.5994\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5785 - val_loss: 0.5525\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5511 - val_loss: 0.5280\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5660 - val_loss: 0.5185\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5339 - val_loss: 0.5144\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5255 - val_loss: 0.5069\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5214 - val_loss: 0.5077\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5223 - val_loss: 0.5025\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5488 - val_loss: 0.5011\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5278 - val_loss: 0.5004\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5219 - val_loss: 0.5032\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5206 - val_loss: 0.4954\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5308 - val_loss: 0.4986\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5598 - val_loss: 0.5037\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5242 - val_loss: 0.4998\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5204 - val_loss: 0.4999\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5211 - val_loss: 0.4991\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5364 - val_loss: 0.4995\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5209 - val_loss: 0.4991\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5232 - val_loss: 0.5019\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5226 - val_loss: 0.5015\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5276 - val_loss: 0.4990\n",
      "121/121 [==============================] - 0s 668us/step - loss: 0.5205\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.5331 - val_loss: 0.5805\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5977 - val_loss: 0.5445\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5488 - val_loss: 0.5487\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5514 - val_loss: 0.5174\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5720 - val_loss: 0.5156\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5310 - val_loss: 0.5261\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5347 - val_loss: 0.5030\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5304 - val_loss: 0.5059\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5280 - val_loss: 0.5382\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5477 - val_loss: 0.5063\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5260 - val_loss: 0.5009\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5348 - val_loss: 0.5010\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5241 - val_loss: 0.4996\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5297 - val_loss: 0.5198\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5333 - val_loss: 0.5031\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5333 - val_loss: 0.5028\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5325 - val_loss: 0.5033\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5250 - val_loss: 0.5029\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5372 - val_loss: 0.5010\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5305 - val_loss: 0.5014\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5272 - val_loss: 0.5023\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5355 - val_loss: 0.5029\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5264 - val_loss: 0.5003\n",
      "121/121 [==============================] - 0s 692us/step - loss: 0.5125\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.5275 - val_loss: 0.7046\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6665 - val_loss: 0.6258\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5918 - val_loss: 0.5936\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5602 - val_loss: 0.5345\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5400 - val_loss: 0.5247\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5294 - val_loss: 0.5439\n",
      "Epoch 7/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5272 - val_loss: 0.5113\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5213 - val_loss: 0.5068\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5260 - val_loss: 0.5364\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5243 - val_loss: 0.5043\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5161 - val_loss: 0.5026\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5194 - val_loss: 0.5007\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5204 - val_loss: 0.5013\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5174 - val_loss: 0.5019\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5169 - val_loss: 0.5143\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5169 - val_loss: 0.5021\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5193 - val_loss: 0.5013\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5183 - val_loss: 0.5017\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5170 - val_loss: 0.5006\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5191 - val_loss: 0.5018\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5176 - val_loss: 0.5035\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5167 - val_loss: 0.5010\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5158 - val_loss: 0.5006\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5185 - val_loss: 0.5002\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5154 - val_loss: 0.5033\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5169 - val_loss: 0.5013\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5175 - val_loss: 0.5071\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5164 - val_loss: 0.5046\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5185 - val_loss: 0.5014\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5171 - val_loss: 0.5022\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5180 - val_loss: 0.5152\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5187 - val_loss: 0.5030\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5172 - val_loss: 0.5028\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5194 - val_loss: 0.5013\n",
      "121/121 [==============================] - 0s 658us/step - loss: 0.5293\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 2.8331 - val_loss: 1.5439\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.2439 - val_loss: 0.9203\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8696 - val_loss: 0.7599\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7568 - val_loss: 0.7016\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7116 - val_loss: 0.6719\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6844 - val_loss: 0.6511\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6633 - val_loss: 0.6331\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6445 - val_loss: 0.6179\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6278 - val_loss: 0.6031\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6125 - val_loss: 0.5895\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5983 - val_loss: 0.5774\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5852 - val_loss: 0.5659\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5729 - val_loss: 0.5551\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5614 - val_loss: 0.5449\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5506 - val_loss: 0.5355\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5407 - val_loss: 0.5268\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5315 - val_loss: 0.5189\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5232 - val_loss: 0.5114\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5155 - val_loss: 0.5045\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5086 - val_loss: 0.4982\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5022 - val_loss: 0.4924\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4962 - val_loss: 0.4873\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4908 - val_loss: 0.4820\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4857 - val_loss: 0.4775\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4811 - val_loss: 0.4731\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4767 - val_loss: 0.4691\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4727 - val_loss: 0.4657\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4690 - val_loss: 0.4620\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4656 - val_loss: 0.4588\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4625 - val_loss: 0.4560\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4593 - val_loss: 0.4537\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4567 - val_loss: 0.4507\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4541 - val_loss: 0.4480\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4516 - val_loss: 0.4466\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4496 - val_loss: 0.4438\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4475 - val_loss: 0.4418\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4452 - val_loss: 0.4404\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4435 - val_loss: 0.4383\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4418 - val_loss: 0.4365\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4399 - val_loss: 0.4356\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4386 - val_loss: 0.4336\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4371 - val_loss: 0.4324\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4357 - val_loss: 0.4310\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4344 - val_loss: 0.4298\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4331 - val_loss: 0.4288\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4320 - val_loss: 0.4278\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4308 - val_loss: 0.4265\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4295 - val_loss: 0.4259\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4284 - val_loss: 0.4247\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4276 - val_loss: 0.4236\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4265 - val_loss: 0.4230\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4256 - val_loss: 0.4217\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4246 - val_loss: 0.4212\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4237 - val_loss: 0.4206\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4229 - val_loss: 0.4193\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4220 - val_loss: 0.4185\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4211 - val_loss: 0.4179\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4202 - val_loss: 0.4171\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4195 - val_loss: 0.4165\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4186 - val_loss: 0.4161\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4179 - val_loss: 0.4147\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4172 - val_loss: 0.4140\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4164 - val_loss: 0.4135\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4157 - val_loss: 0.4126\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4151 - val_loss: 0.4121\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4143 - val_loss: 0.4117\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4136 - val_loss: 0.4111\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4130 - val_loss: 0.4102\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4123 - val_loss: 0.4095\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4117 - val_loss: 0.4090\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4110 - val_loss: 0.4085\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4104 - val_loss: 0.4085\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4098 - val_loss: 0.4074\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4094 - val_loss: 0.4067\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4086 - val_loss: 0.4065\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4082 - val_loss: 0.4059\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4073 - val_loss: 0.4061\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4069 - val_loss: 0.4049\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4063 - val_loss: 0.4044\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4058 - val_loss: 0.4036\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4053 - val_loss: 0.4033\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4047 - val_loss: 0.4030\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4042 - val_loss: 0.4020\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4036 - val_loss: 0.4019\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4031 - val_loss: 0.4011\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4025 - val_loss: 0.4005\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4020 - val_loss: 0.4000\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4015 - val_loss: 0.3997\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4011 - val_loss: 0.3992\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4003 - val_loss: 0.3987\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3999 - val_loss: 0.3981\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3995 - val_loss: 0.3977\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3990 - val_loss: 0.3974\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3981 - val_loss: 0.3980\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3980 - val_loss: 0.3965\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3975 - val_loss: 0.3959\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3971 - val_loss: 0.3958\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3966 - val_loss: 0.3953\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3961 - val_loss: 0.3946\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3957 - val_loss: 0.3943\n",
      "121/121 [==============================] - 0s 753us/step - loss: 0.4076\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 2.5342 - val_loss: 1.2202\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.0527 - val_loss: 0.8442\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8232 - val_loss: 0.7466\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7551 - val_loss: 0.7035\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7201 - val_loss: 0.6749\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6938 - val_loss: 0.6526\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6717 - val_loss: 0.6324\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6515 - val_loss: 0.6151\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6332 - val_loss: 0.5992\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6162 - val_loss: 0.5836\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6012 - val_loss: 0.5702\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5869 - val_loss: 0.5577\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5738 - val_loss: 0.5464\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5618 - val_loss: 0.5365\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5506 - val_loss: 0.5257\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5399 - val_loss: 0.5170\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5304 - val_loss: 0.5083\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5215 - val_loss: 0.5008\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5131 - val_loss: 0.4931\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5056 - val_loss: 0.4868\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4988 - val_loss: 0.4807\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4923 - val_loss: 0.4757\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4865 - val_loss: 0.4699\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4811 - val_loss: 0.4654\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4761 - val_loss: 0.4612\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4715 - val_loss: 0.4574\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4677 - val_loss: 0.4539\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4637 - val_loss: 0.4506\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4603 - val_loss: 0.4480\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4571 - val_loss: 0.4451\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4542 - val_loss: 0.4424\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4513 - val_loss: 0.4404\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4489 - val_loss: 0.4380\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4465 - val_loss: 0.4360\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4443 - val_loss: 0.4340\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4423 - val_loss: 0.4325\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4403 - val_loss: 0.4313\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4385 - val_loss: 0.4294\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4368 - val_loss: 0.4277\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4348 - val_loss: 0.4269\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4337 - val_loss: 0.4253\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4321 - val_loss: 0.4241\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4306 - val_loss: 0.4225\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4294 - val_loss: 0.4212\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4280 - val_loss: 0.4203\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4268 - val_loss: 0.4195\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4255 - val_loss: 0.4180\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4244 - val_loss: 0.4176\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4230 - val_loss: 0.4162\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4222 - val_loss: 0.4152\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4208 - val_loss: 0.4146\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4201 - val_loss: 0.4136\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4190 - val_loss: 0.4127\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4180 - val_loss: 0.4126\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4171 - val_loss: 0.4112\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4162 - val_loss: 0.4101\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4153 - val_loss: 0.4093\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4143 - val_loss: 0.4086\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4135 - val_loss: 0.4079\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4128 - val_loss: 0.4072\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4117 - val_loss: 0.4061\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4109 - val_loss: 0.4057\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4101 - val_loss: 0.4048\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4093 - val_loss: 0.4045\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4086 - val_loss: 0.4036\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4079 - val_loss: 0.4030\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4071 - val_loss: 0.4023\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4063 - val_loss: 0.4016\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4056 - val_loss: 0.4011\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4049 - val_loss: 0.4007\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4042 - val_loss: 0.3999\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4034 - val_loss: 0.3997\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4029 - val_loss: 0.3988\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4021 - val_loss: 0.3982\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4015 - val_loss: 0.3977\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4006 - val_loss: 0.3968\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4002 - val_loss: 0.3970\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3991 - val_loss: 0.3970\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3991 - val_loss: 0.3956\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3983 - val_loss: 0.3951\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3977 - val_loss: 0.3947\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3969 - val_loss: 0.3960\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3965 - val_loss: 0.3935\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3959 - val_loss: 0.3932\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3952 - val_loss: 0.3928\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3948 - val_loss: 0.3917\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3940 - val_loss: 0.3914\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3936 - val_loss: 0.3911\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3930 - val_loss: 0.3905\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3925 - val_loss: 0.3901\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3919 - val_loss: 0.3894\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3913 - val_loss: 0.3888\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3908 - val_loss: 0.3886\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3903 - val_loss: 0.3886\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3898 - val_loss: 0.3878\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3893 - val_loss: 0.3869\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3886 - val_loss: 0.3870\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3882 - val_loss: 0.3862\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3876 - val_loss: 0.3855\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3873 - val_loss: 0.3852\n",
      "121/121 [==============================] - 0s 827us/step - loss: 0.3922\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 2.5793 - val_loss: 1.3808\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.1905 - val_loss: 0.9700\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.9154 - val_loss: 0.8260\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8089 - val_loss: 0.7578\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7529 - val_loss: 0.7173\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7157 - val_loss: 0.6885\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6868 - val_loss: 0.6634\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6621 - val_loss: 0.6416\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6404 - val_loss: 0.6231\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6208 - val_loss: 0.6048\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6040 - val_loss: 0.5892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5885 - val_loss: 0.5753\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5747 - val_loss: 0.5628\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5618 - val_loss: 0.5523\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5507 - val_loss: 0.5416\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5397 - val_loss: 0.5321\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5309 - val_loss: 0.5231\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5230 - val_loss: 0.5160\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5155 - val_loss: 0.5091\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5090 - val_loss: 0.5034\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5029 - val_loss: 0.4984\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4977 - val_loss: 0.4935\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4926 - val_loss: 0.4884\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4885 - val_loss: 0.4846\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4843 - val_loss: 0.4814\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4804 - val_loss: 0.4779\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4774 - val_loss: 0.4751\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4738 - val_loss: 0.4720\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4710 - val_loss: 0.4695\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4681 - val_loss: 0.4672\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4656 - val_loss: 0.4648\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4630 - val_loss: 0.4628\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4607 - val_loss: 0.4605\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4584 - val_loss: 0.4577\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4564 - val_loss: 0.4560\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4541 - val_loss: 0.4545\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4523 - val_loss: 0.4523\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4502 - val_loss: 0.4514\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4488 - val_loss: 0.4494\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4471 - val_loss: 0.4475\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4455 - val_loss: 0.4461\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4438 - val_loss: 0.4444\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4423 - val_loss: 0.4429\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4406 - val_loss: 0.4416\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4391 - val_loss: 0.4405\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4379 - val_loss: 0.4388\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4365 - val_loss: 0.4378\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4351 - val_loss: 0.4367\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4338 - val_loss: 0.4352\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4325 - val_loss: 0.4347\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4313 - val_loss: 0.4331\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4301 - val_loss: 0.4321\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4289 - val_loss: 0.4315\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4278 - val_loss: 0.4307\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4267 - val_loss: 0.4287\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4258 - val_loss: 0.4279\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4245 - val_loss: 0.4269\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4236 - val_loss: 0.4259\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4227 - val_loss: 0.4248\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4215 - val_loss: 0.4240\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4209 - val_loss: 0.4233\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4198 - val_loss: 0.4223\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4190 - val_loss: 0.4215\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4181 - val_loss: 0.4211\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4173 - val_loss: 0.4199\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4163 - val_loss: 0.4193\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4158 - val_loss: 0.4190\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4148 - val_loss: 0.4186\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4141 - val_loss: 0.4174\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4134 - val_loss: 0.4169\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4127 - val_loss: 0.4160\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4118 - val_loss: 0.4152\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4111 - val_loss: 0.4146\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4104 - val_loss: 0.4139\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4097 - val_loss: 0.4131\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4089 - val_loss: 0.4125\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4080 - val_loss: 0.4132\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4076 - val_loss: 0.4116\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4068 - val_loss: 0.4108\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4061 - val_loss: 0.4100\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4054 - val_loss: 0.4095\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4046 - val_loss: 0.4097\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4041 - val_loss: 0.4087\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4034 - val_loss: 0.4077\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4027 - val_loss: 0.4072\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4022 - val_loss: 0.4066\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4016 - val_loss: 0.4060\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4008 - val_loss: 0.4057\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4003 - val_loss: 0.4049\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3998 - val_loss: 0.4045\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3991 - val_loss: 0.4037\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3984 - val_loss: 0.4033\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3978 - val_loss: 0.4035\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3974 - val_loss: 0.4023\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3968 - val_loss: 0.4021\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3963 - val_loss: 0.4014\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3957 - val_loss: 0.4010\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3950 - val_loss: 0.4008\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3945 - val_loss: 0.3999\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3939 - val_loss: 0.3994\n",
      "121/121 [==============================] - 0s 883us/step - loss: 0.4194\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 2.5414 - val_loss: 1.3654\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.1750 - val_loss: 0.9200\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.9311 - val_loss: 0.8186\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8386 - val_loss: 0.7654\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7863 - val_loss: 0.7304\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7509 - val_loss: 0.7038\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7241 - val_loss: 0.6826\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7021 - val_loss: 0.6645\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6836 - val_loss: 0.6490\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6678 - val_loss: 0.6353\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6538 - val_loss: 0.6233\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6412 - val_loss: 0.6123\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6297 - val_loss: 0.6024\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6193 - val_loss: 0.5934\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6097 - val_loss: 0.5849\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6008 - val_loss: 0.5770\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5924 - val_loss: 0.5698\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5848 - val_loss: 0.5629\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5775 - val_loss: 0.5566\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5708 - val_loss: 0.5506\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5644 - val_loss: 0.5451\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5584 - val_loss: 0.5398\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5527 - val_loss: 0.5349\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5474 - val_loss: 0.5301\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5424 - val_loss: 0.5257\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5376 - val_loss: 0.5215\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5332 - val_loss: 0.5177\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5288 - val_loss: 0.5137\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5248 - val_loss: 0.5100\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5210 - val_loss: 0.5066\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5170 - val_loss: 0.5038\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5136 - val_loss: 0.5003\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5102 - val_loss: 0.4969\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5066 - val_loss: 0.4946\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5037 - val_loss: 0.4912\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5006 - val_loss: 0.4885\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4975 - val_loss: 0.4861\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4948 - val_loss: 0.4835\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4922 - val_loss: 0.4809\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4894 - val_loss: 0.4788\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4871 - val_loss: 0.4763\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4846 - val_loss: 0.4742\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4823 - val_loss: 0.4721\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4801 - val_loss: 0.4701\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4779 - val_loss: 0.4681\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4759 - val_loss: 0.4663\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4738 - val_loss: 0.4644\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4717 - val_loss: 0.4627\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4698 - val_loss: 0.4609\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4680 - val_loss: 0.4592\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4662 - val_loss: 0.4577\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4645 - val_loss: 0.4559\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4628 - val_loss: 0.4545\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4612 - val_loss: 0.4532\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4597 - val_loss: 0.4514\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4581 - val_loss: 0.4500\n",
      "Epoch 57/100\n",
      "233/242 [===========================>..] - ETA: 0s - loss: 0.456"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-4afbf9f191dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mrnd_search_cv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomizedSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeras_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_distribs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m rnd_search_cv.fit(X_train, y_train, epochs = 100,\n\u001b[0m\u001b[1;32m      3\u001b[0m                  \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                  callbacks = [keras.callbacks.EarlyStopping(patience = 10)])\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    734\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 736\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1527\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1528\u001b[0m         \u001b[0;34m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1529\u001b[0;31m         evaluate_candidates(ParameterSampler(\n\u001b[0m\u001b[1;32m   1530\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m             random_state=self.random_state))\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    706\u001b[0m                               n_splits, n_candidates, n_candidates * n_splits))\n\u001b[1;32m    707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m                 out = parallel(delayed(_fit_and_score)(clone(base_estimator),\n\u001b[0m\u001b[1;32m    709\u001b[0m                                                        \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m                                                        \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1049\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    864\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 866\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    867\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    782\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    783\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 784\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    785\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    529\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/keras/wrappers/scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0mfit_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1265\u001b[0m           \u001b[0mepoch_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1268\u001b[0m         \u001b[0mtraining_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    412\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m       \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1043\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_finalize_progbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1044\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_finalize_progbar\u001b[0;34m(self, logs, counter)\u001b[0m\n\u001b[1;32m   1114\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcounter\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1116\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, current, values, finalize)\u001b[0m\n\u001b[1;32m    913\u001b[0m       \u001b[0mprev_total_width\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_total_width\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamic_display\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\b'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mprev_total_width\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/ipykernel/iostream.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, string)\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0mis_child\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_master_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m             \u001b[0;31m# only touch the buffer in the IO thread to avoid races\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    405\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_child\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m                 \u001b[0;31m# mp.Pool cannot be trusted to flush promptly (or ever),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/ipykernel/iostream.py\u001b[0m in \u001b[0;36mschedule\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    203\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_events\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0;31m# wake event thread (message content is ignored)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_pipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, data, flags, copy, track, routing_id, group)\u001b[0m\n\u001b[1;32m    414\u001b[0m                                  copy_threshold=self.copy_threshold)\n\u001b[1;32m    415\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSocket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msend_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg_parts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._send_copy\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rnd_search_cv = RandomizedSearchCV(keras_reg, param_distribs, n_iter = 10, cv = 3)\n",
    "rnd_search_cv.fit(X_train, y_train, epochs = 100,\n",
    "                 validation_data = (X_valid, y_valid),\n",
    "                 callbacks = [keras.callbacks.EarlyStopping(patience = 10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_search_cv.best_params_\n",
    "rnd_serach_cv.best_score_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
