{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('ln', LogisticRegression()),\n",
       "                             ('rf', RandomForestClassifier()), ('svc', SVC())])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# composed of 3 diverse classifiers\n",
    "# hard voting \n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "log_clf = LogisticRegression()\n",
    "rnd_clf = RandomForestClassifier()\n",
    "svm_clf = SVC()\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators = [('ln', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
    "    voting = 'hard')\n",
    "\n",
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.864\n",
      "RandomForestClassifier 0.904\n",
      "SVC 0.896\n",
      "VotingClassifier 0.904\n"
     ]
    }
   ],
   "source": [
    "# accuracy test\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))\n",
    "    \n",
    "# voting had the most accurate result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.864\n",
      "RandomForestClassifier 0.896\n",
      "SVC 0.896\n",
      "VotingClassifier 0.92\n"
     ]
    }
   ],
   "source": [
    "# Soft voting\n",
    "log_clf = LogisticRegression(solver=\"lbfgs\", random_state=42)\n",
    "rnd_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "svm_clf = SVC(gamma=\"scale\", probability=True, random_state=42)\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
    "    voting='soft')\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging and Pasting\n",
    "\n",
    "Bagging: sampling is performed with replacement \\\n",
    "Pasting: sampling is performed without replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(), n_estimators = 500, # ensemble of 500 Decision Tree classifiers\n",
    "    max_samples = 100, bootstrap = True, n_jobs = -1) # each is trained on 100 training instances, with replacement\n",
    "# n_jobs = -1 : use all available cores\n",
    "\n",
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred = bag_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.896"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Out-of-Bag (oob) Evaluation = remaining instances\n",
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(), n_estimators = 500,\n",
    "    bootstrap = True, n_jobs = -1, oob_score = True) # request automatic oob evaluation after training\n",
    "\n",
    "bag_clf.fit(X_train, y_train)\n",
    "bag_clf.oob_score_\n",
    "\n",
    "# according to oob evaluation, BaggingClassifier is likely to achieve about 89.6% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.912"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_predict = bag_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.38121547, 0.61878453],\n",
       "       [0.31666667, 0.68333333],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.00552486, 0.99447514],\n",
       "       [0.0631068 , 0.9368932 ],\n",
       "       [0.45505618, 0.54494382],\n",
       "       [0.01612903, 0.98387097],\n",
       "       [1.        , 0.        ],\n",
       "       [0.98863636, 0.01136364],\n",
       "       [0.79532164, 0.20467836],\n",
       "       [0.01086957, 0.98913043],\n",
       "       [0.775     , 0.225     ],\n",
       "       [0.85628743, 0.14371257],\n",
       "       [0.96666667, 0.03333333],\n",
       "       [0.05487805, 0.94512195],\n",
       "       [0.        , 1.        ],\n",
       "       [0.99408284, 0.00591716],\n",
       "       [0.95      , 0.05      ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.02116402, 0.97883598],\n",
       "       [0.27225131, 0.72774869],\n",
       "       [0.93181818, 0.06818182],\n",
       "       [1.        , 0.        ],\n",
       "       [0.97727273, 0.02272727],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.70899471, 0.29100529],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.12903226, 0.87096774],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.41752577, 0.58247423],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.25263158, 0.74736842],\n",
       "       [0.36413043, 0.63586957],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.02105263, 0.97894737],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01639344, 0.98360656],\n",
       "       [0.9798995 , 0.0201005 ],\n",
       "       [0.92592593, 0.07407407],\n",
       "       [0.98      , 0.02      ],\n",
       "       [0.97837838, 0.02162162],\n",
       "       [0.        , 1.        ],\n",
       "       [0.05376344, 0.94623656],\n",
       "       [0.98421053, 0.01578947],\n",
       "       [0.00552486, 0.99447514],\n",
       "       [0.        , 1.        ],\n",
       "       [0.00546448, 0.99453552],\n",
       "       [0.98830409, 0.01169591],\n",
       "       [0.80952381, 0.19047619],\n",
       "       [0.38586957, 0.61413043],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.67204301, 0.32795699],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.82795699, 0.17204301],\n",
       "       [1.        , 0.        ],\n",
       "       [0.62244898, 0.37755102],\n",
       "       [0.11734694, 0.88265306],\n",
       "       [0.62643678, 0.37356322],\n",
       "       [0.89054726, 0.10945274],\n",
       "       [0.        , 1.        ],\n",
       "       [0.11235955, 0.88764045],\n",
       "       [0.86857143, 0.13142857],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.99507389, 0.00492611],\n",
       "       [0.        , 1.        ],\n",
       "       [0.03045685, 0.96954315],\n",
       "       [0.03488372, 0.96511628],\n",
       "       [0.28061224, 0.71938776],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.84343434, 0.15656566],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.2974359 , 0.7025641 ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.0052356 , 0.9947644 ],\n",
       "       [0.94642857, 0.05357143],\n",
       "       [0.73006135, 0.26993865],\n",
       "       [0.00578035, 0.99421965],\n",
       "       [1.        , 0.        ],\n",
       "       [0.20114943, 0.79885057],\n",
       "       [0.68888889, 0.31111111],\n",
       "       [0.        , 1.        ],\n",
       "       [0.01515152, 0.98484848],\n",
       "       [0.50526316, 0.49473684],\n",
       "       [0.99408284, 0.00591716],\n",
       "       [0.03141361, 0.96858639],\n",
       "       [0.99447514, 0.00552486],\n",
       "       [0.30337079, 0.69662921],\n",
       "       [0.47337278, 0.52662722],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00549451, 0.99450549],\n",
       "       [0.99444444, 0.00555556],\n",
       "       [0.24456522, 0.75543478],\n",
       "       [0.90751445, 0.09248555],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.82417582, 0.17582418],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01851852, 0.98148148],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.99447514, 0.00552486],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00540541, 0.99459459],\n",
       "       [0.96111111, 0.03888889],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01734104, 0.98265896],\n",
       "       [0.2184466 , 0.7815534 ],\n",
       "       [0.95897436, 0.04102564],\n",
       "       [0.32748538, 0.67251462],\n",
       "       [0.9939759 , 0.0060241 ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.00564972, 0.99435028],\n",
       "       [0.68181818, 0.31818182],\n",
       "       [0.3372093 , 0.6627907 ],\n",
       "       [0.43434343, 0.56565657],\n",
       "       [0.86338798, 0.13661202],\n",
       "       [0.965     , 0.035     ],\n",
       "       [0.05612245, 0.94387755],\n",
       "       [0.82258065, 0.17741935],\n",
       "       [0.00512821, 0.99487179],\n",
       "       [0.        , 1.        ],\n",
       "       [0.02352941, 0.97647059],\n",
       "       [0.98148148, 0.01851852],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00555556, 0.99444444],\n",
       "       [0.        , 1.        ],\n",
       "       [0.0052356 , 0.9947644 ],\n",
       "       [0.00552486, 0.99447514],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.96276596, 0.03723404],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.9939759 , 0.0060241 ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.33152174, 0.66847826],\n",
       "       [0.31372549, 0.68627451],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.29787234, 0.70212766],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01117318, 0.98882682],\n",
       "       [0.        , 1.        ],\n",
       "       [0.9787234 , 0.0212766 ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.015625  , 0.984375  ],\n",
       "       [0.62295082, 0.37704918],\n",
       "       [0.88826816, 0.11173184],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.06666667, 0.93333333],\n",
       "       [1.        , 0.        ],\n",
       "       [0.05405405, 0.94594595],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.03932584, 0.96067416],\n",
       "       [1.        , 0.        ],\n",
       "       [0.92771084, 0.07228916],\n",
       "       [0.76804124, 0.23195876],\n",
       "       [0.57471264, 0.42528736],\n",
       "       [0.        , 1.        ],\n",
       "       [0.10429448, 0.89570552],\n",
       "       [1.        , 0.        ],\n",
       "       [0.94240838, 0.05759162],\n",
       "       [0.97837838, 0.02162162],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01149425, 0.98850575],\n",
       "       [0.        , 1.        ],\n",
       "       [0.45410628, 0.54589372],\n",
       "       [0.86666667, 0.13333333],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.02116402, 0.97883598],\n",
       "       [0.        , 1.        ],\n",
       "       [0.96551724, 0.03448276],\n",
       "       [0.        , 1.        ],\n",
       "       [0.26229508, 0.73770492],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.98979592, 0.01020408],\n",
       "       [0.79381443, 0.20618557],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.0802139 , 0.9197861 ],\n",
       "       [0.99462366, 0.00537634],\n",
       "       [0.02840909, 0.97159091],\n",
       "       [0.        , 1.        ],\n",
       "       [0.05649718, 0.94350282],\n",
       "       [0.99444444, 0.00555556],\n",
       "       [0.83333333, 0.16666667],\n",
       "       [0.        , 1.        ],\n",
       "       [0.85714286, 0.14285714],\n",
       "       [0.99465241, 0.00534759],\n",
       "       [0.22162162, 0.77837838],\n",
       "       [0.1754386 , 0.8245614 ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.23737374, 0.76262626],\n",
       "       [0.95675676, 0.04324324],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.98882682, 0.01117318],\n",
       "       [0.        , 1.        ],\n",
       "       [0.48691099, 0.51308901],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00529101, 0.99470899],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.11413043, 0.88586957],\n",
       "       [0.13953488, 0.86046512],\n",
       "       [0.9893617 , 0.0106383 ],\n",
       "       [0.01036269, 0.98963731],\n",
       "       [1.        , 0.        ],\n",
       "       [0.37988827, 0.62011173],\n",
       "       [0.08928571, 0.91071429],\n",
       "       [0.52173913, 0.47826087],\n",
       "       [0.57541899, 0.42458101],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.63316583, 0.36683417],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.19371728, 0.80628272],\n",
       "       [0.80952381, 0.19047619],\n",
       "       [0.07909605, 0.92090395],\n",
       "       [1.        , 0.        ],\n",
       "       [0.84656085, 0.15343915],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.09137056, 0.90862944],\n",
       "       [0.02094241, 0.97905759],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.93678161, 0.06321839],\n",
       "       [0.17877095, 0.82122905],\n",
       "       [0.97073171, 0.02926829],\n",
       "       [0.01104972, 0.98895028],\n",
       "       [0.55865922, 0.44134078],\n",
       "       [0.07692308, 0.92307692],\n",
       "       [0.99479167, 0.00520833],\n",
       "       [0.8342246 , 0.1657754 ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.9947644 , 0.0052356 ],\n",
       "       [0.95979899, 0.04020101],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.32571429, 0.67428571],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.01156069, 0.98843931],\n",
       "       [0.85051546, 0.14948454],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.73626374, 0.26373626],\n",
       "       [0.96907216, 0.03092784],\n",
       "       [1.        , 0.        ],\n",
       "       [0.77456647, 0.22543353],\n",
       "       [0.52352941, 0.47647059],\n",
       "       [0.        , 1.        ],\n",
       "       [0.9017341 , 0.0982659 ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.88      , 0.12      ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.81025641, 0.18974359],\n",
       "       [0.10215054, 0.89784946],\n",
       "       [0.47222222, 0.52777778],\n",
       "       [0.26744186, 0.73255814],\n",
       "       [0.        , 1.        ],\n",
       "       [0.83937824, 0.16062176],\n",
       "       [0.83977901, 0.16022099],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.99404762, 0.00595238],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.01724138, 0.98275862],\n",
       "       [0.96855346, 0.03144654],\n",
       "       [0.92045455, 0.07954545],\n",
       "       [1.        , 0.        ],\n",
       "       [0.50515464, 0.49484536],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.99484536, 0.00515464],\n",
       "       [0.01156069, 0.98843931],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.97740113, 0.02259887],\n",
       "       [0.        , 1.        ],\n",
       "       [0.07253886, 0.92746114],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.99404762, 0.00595238],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.09574468, 0.90425532],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.44886364, 0.55113636],\n",
       "       [0.08762887, 0.91237113],\n",
       "       [0.17346939, 0.82653061],\n",
       "       [1.        , 0.        ],\n",
       "       [0.98924731, 0.01075269],\n",
       "       [0.21465969, 0.78534031],\n",
       "       [0.96842105, 0.03157895],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.97109827, 0.02890173],\n",
       "       [0.31073446, 0.68926554],\n",
       "       [0.97752809, 0.02247191],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.03108808, 0.96891192],\n",
       "       [0.99465241, 0.00534759],\n",
       "       [1.        , 0.        ],\n",
       "       [0.025     , 0.975     ],\n",
       "       [0.62564103, 0.37435897]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# oob decision function for each training instance\n",
    "\n",
    "bag_clf.oob_decision_function_\n",
    "#[negative class, positive class]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rnd_clf = RandomForestClassifier(n_estimators = 500, max_leaf_nodes = 16, n_jobs = -1)\n",
    "rnd_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = rnd_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can be written in this way:\n",
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(splitter = \"random\", max_leaf_nodes = 16),\n",
    "    n_estimators = 500, max_samples = 1.0, bootstrap = True, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance\n",
    "\n",
    "In random forest, feature's importance is measured by looking at how much the tree nodes that use that feature reduce impurity on average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal length (cm) 0.10683373284303369\n",
      "sepal width (cm) 0.026560042227229505\n",
      "petal length (cm) 0.40142169192197735\n",
      "petal width (cm) 0.4651845330077594\n"
     ]
    }
   ],
   "source": [
    "# using iris data\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "rnd_clf = RandomForestClassifier(n_estimators = 500, n_jobs = -1)\n",
    "rnd_clf.fit(iris[\"data\"], iris[\"target\"])\n",
    "for name, score in zip(iris[\"feature_names\"], rnd_clf.feature_importances_):\n",
    "    print(name, score)\n",
    "    \n",
    "# petal length (44%) and petal width (42%) are the most important"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting\n",
    "\n",
    "Ensemble method that combine weak learners into a strong learner.\n",
    "\n",
    "### AdaBoost\n",
    "\n",
    "Pay more attention to the training instances that the precedessor underfitted --> new predictors focusing more and more on the hard cases\n",
    "- Adaboost adds predictors to the ensemble gradually making it better. Tweak the instance weight at every iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1),\n",
       "                   learning_rate=0.5, n_estimators=200)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada_clf = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(max_depth = 1), n_estimators = 200,\n",
    "    algorithm = \"SAMME.R\", learning_rate = 0.5) # SAMME = AdaBoost when there are only 2 classes, R = \"real\"\n",
    "ada_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting\n",
    "\n",
    "Gradient Boosting adds predictors sequentially to an ensemble, each one correcting its precessor. \n",
    "- Difference from the AdaBoost is that the method tries to fit the new predictor to the residual errors made by the previous predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.75026781])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gradient Tree Boosting\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 1) - 0.5\n",
    "y = 3*X[:, 0]**2 + 0.05 * np.random.randn(100)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg1 = DecisionTreeRegressor(max_depth = 2)\n",
    "tree_reg1.fit(X, y)\n",
    "\n",
    "y2 = y - tree_reg1.predict(X) # residual error made by the first predictor\n",
    "tree_reg2 = DecisionTreeRegressor(max_depth = 2)\n",
    "tree_reg2.fit(X, y2)\n",
    "\n",
    "y3 = y2 - tree_reg2.predict(X) # residual error made by the second predictor\n",
    "tree_reg3 = DecisionTreeRegressor(max_depth = 2)\n",
    "tree_reg3.fit(X, y3)\n",
    "\n",
    "X_new = np.array([[0.8]])\n",
    "y_pred = sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(learning_rate=1.0, max_depth=2, n_estimators=3)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor # simpler way\n",
    "\n",
    "gbrt = GradientBoostingRegressor(max_depth = 2, n_estimators = 3, learning_rate = 1.0)\n",
    "gbrt.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(max_depth=2, n_estimators=120)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y)\n",
    "\n",
    "gbrt = GradientBoostingRegressor(max_depth = 2, n_estimators = 120)\n",
    "gbrt.fit(X, y)\n",
    "\n",
    "errors = [mean_squared_error(y_val, y_pred)\n",
    "         for y_pred in gbrt.staged_predict(X_val)] \n",
    "# staged_predict = returns an iterator over the predictions made by the ensemble at each stage of training\n",
    "bst_n_estimators = np.argmin(errors) + 1\n",
    "\n",
    "gbrt_best = GradientBoostingRegressor(max_depth = 2, n_estimators = bst_n_estimators)\n",
    "gbrt_best.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# early stopping\n",
    "\n",
    "grbt = GradientBoostingRegressor(max_depth = 2, warm_start = True)\n",
    "# warm_start = keep existing trees when fit() method is called\n",
    "\n",
    "min_val_error = float(\"inf\")\n",
    "error_going_up = 0\n",
    "for n_estimators in range(1, 120):\n",
    "    grbt.n_estimators = n_estimators\n",
    "    grbt.fit(X_train, y_train)\n",
    "    y_pred = grbt.predict(X_val)\n",
    "    val_error = mean_squared_error(y_val, y_pred)\n",
    "    if val_error < min_val_error:\n",
    "        min_val_error = val_error\n",
    "        error_going_up = 0\n",
    "    else:\n",
    "        error_going_up += 1\n",
    "        if error_going_up == 5:  # does not improve 5 iterations in a row\n",
    "            break # early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n"
     ]
    }
   ],
   "source": [
    "print(gbrt.n_estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum validation MSE: 0.002750279033345716\n"
     ]
    }
   ],
   "source": [
    "print(\"Minimum validation MSE:\", min_val_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
